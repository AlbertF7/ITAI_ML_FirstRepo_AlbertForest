{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) and Data Visualization\n",
    "## Module 4, Lab 2: Understanding Your Data\n",
    "\n",
    "Exploratory Data Analysis (EDA) is one of the most critical steps in any machine learning project. Before building models, you need to understand your data thoroughly. This lab will teach you how to explore datasets, identify patterns, and create meaningful visualizations.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "- Load and examine datasets using pandas\n",
    "- Identify data quality issues (missing values, duplicates, outliers)\n",
    "- Calculate and interpret summary statistics\n",
    "- Create effective visualizations using matplotlib and seaborn\n",
    "- Draw insights from data exploration\n",
    "\n",
    "### Business Problem\n",
    "We'll analyze a customer dataset to understand purchasing behavior and demographics. This type of analysis helps businesses make data-driven decisions about marketing, product development, and customer segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --upgrade pip\n",
    "!pip install pandas numpy matplotlib seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "We'll create a realistic customer dataset for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic customer dataset\n",
    "np.random.seed(42)\n",
    "n_customers = 1000\n",
    "\n",
    "# Generate customer data\n",
    "customer_data = {\n",
    "    'customer_id': range(1, n_customers + 1),\n",
    "    'age': np.random.normal(40, 15, n_customers).astype(int),\n",
    "    'gender': np.random.choice(['Male', 'Female', 'Other'], n_customers, p=[0.48, 0.50, 0.02]),\n",
    "    'income': np.random.lognormal(10.5, 0.5, n_customers),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], \n",
    "                                 n_customers, p=[0.3, 0.4, 0.25, 0.05]),\n",
    "    'city_tier': np.random.choice(['Tier 1', 'Tier 2', 'Tier 3'], \n",
    "                                 n_customers, p=[0.3, 0.4, 0.3]),\n",
    "    'years_as_customer': np.random.exponential(3, n_customers),\n",
    "    'total_purchases': np.random.poisson(12, n_customers),\n",
    "    'avg_order_value': np.random.gamma(2, 50, n_customers),\n",
    "    'satisfaction_score': np.random.normal(7.5, 1.5, n_customers)\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(customer_data)\n",
    "\n",
    "# Add some realistic constraints\n",
    "df['age'] = np.clip(df['age'], 18, 80)\n",
    "df['income'] = np.clip(df['income'], 20000, 200000)\n",
    "df['years_as_customer'] = np.clip(df['years_as_customer'], 0, 15)\n",
    "df['satisfaction_score'] = np.clip(df['satisfaction_score'], 1, 10)\n",
    "\n",
    "# Calculate total spending\n",
    "df['total_spending'] = df['total_purchases'] * df['avg_order_value']\n",
    "\n",
    "# Introduce some missing values (realistic scenario)\n",
    "missing_indices = np.random.choice(df.index, size=50, replace=False)\n",
    "df.loc[missing_indices, 'satisfaction_score'] = np.nan\n",
    "\n",
    "# Add some duplicates (data quality issue)\n",
    "duplicate_rows = df.sample(5).copy()\n",
    "df = pd.concat([df, duplicate_rows], ignore_index=True)\n",
    "\n",
    "print(f\"Dataset created with {len(df)} customers\")\n",
    "print(f\"Dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initial Data Exploration\n",
    "Let's start by getting familiar with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nLast 5 rows of the dataset:\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic information about the dataset\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nDataset Shape:\")\n",
    "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
    "\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nNumerical Columns:\")\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(numerical_cols)\n",
    "\n",
    "print(\"\\nCategorical Columns:\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Quality Assessment\n",
    "Before analyzing the data, we need to identify and understand data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Missing Percentage': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Unique rows: {len(df.drop_duplicates())}\")\n",
    "print(f\"Duplicate rows: {len(df) - len(df.drop_duplicates())}\")\n",
    "\n",
    "if len(df) != len(df.drop_duplicates()):\n",
    "    print(\"\\nDuplicate rows found:\")\n",
    "    duplicates = df[df.duplicated(keep=False)]\n",
    "    print(duplicates.sort_values('customer_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers using IQR method\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "print(\"Outlier Detection (using IQR method):\")\n",
    "for col in ['age', 'income', 'total_spending']:\n",
    "    outliers, lower, upper = detect_outliers(df, col)\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Normal range: {lower:.2f} to {upper:.2f}\")\n",
    "    print(f\"  Number of outliers: {len(outliers)}\")\n",
    "    print(f\"  Percentage of outliers: {len(outliers)/len(df)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Summary Statistics\n",
    "Let's calculate and interpret summary statistics for our numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic summary statistics\n",
    "print(\"Summary Statistics for Numerical Variables:\")\n",
    "summary_stats = df.describe()\n",
    "display(summary_stats.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional statistics\n",
    "print(\"Additional Statistics:\")\n",
    "additional_stats = pd.DataFrame({\n",
    "    'Skewness': df[numerical_cols].skew(),\n",
    "    'Kurtosis': df[numerical_cols].kurtosis(),\n",
    "    'Variance': df[numerical_cols].var()\n",
    "})\n",
    "display(additional_stats.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary for categorical variables\n",
    "print(\"Summary for Categorical Variables:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    value_counts = df[col].value_counts()\n",
    "    percentages = df[col].value_counts(normalize=True) * 100\n",
    "    summary = pd.DataFrame({\n",
    "        'Count': value_counts,\n",
    "        'Percentage': percentages\n",
    "    })\n",
    "    print(summary.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Visualization\n",
    "Now let's create visualizations to better understand our data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Distribution of Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for numerical variables\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "numerical_vars = ['age', 'income', 'years_as_customer', 'total_purchases', 'avg_order_value', 'total_spending']\n",
    "\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    axes[i].hist(df[var].dropna(), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[i].set_title(f'Distribution of {var.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_xlabel(var.replace(\"_\", \" \").title())\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots to identify outliers\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, var in enumerate(numerical_vars):\n",
    "    axes[i].boxplot(df[var].dropna())\n",
    "    axes[i].set_title(f'Box Plot of {var.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_ylabel(var.replace(\"_\", \" \").title())\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Categorical Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plots for categorical variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, var in enumerate(categorical_cols):\n",
    "    value_counts = df[var].value_counts()\n",
    "    axes[i].bar(value_counts.index, value_counts.values, alpha=0.7)\n",
    "    axes[i].set_title(f'Distribution of {var.replace(\"_\", \" \").title()}')\n",
    "    axes[i].set_xlabel(var.replace(\"_\", \" \").title())\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie charts for categorical variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, var in enumerate(categorical_cols):\n",
    "    value_counts = df[var].value_counts()\n",
    "    axes[i].pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[i].set_title(f'Proportion of {var.replace(\"_\", \" \").title()}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, fmt='.2f')\n",
    "plt.title('Correlation Matrix of Numerical Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print strong correlations\n",
    "print(\"Strong Correlations (|r| > 0.5):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) > 0.5:\n",
    "            print(f\"{correlation_matrix.columns[i]} vs {correlation_matrix.columns[j]}: {corr_value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for key relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Income vs Total Spending\n",
    "axes[0, 0].scatter(df['income'], df['total_spending'], alpha=0.6)\n",
    "axes[0, 0].set_xlabel('Income')\n",
    "axes[0, 0].set_ylabel('Total Spending')\n",
    "axes[0, 0].set_title('Income vs Total Spending')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Age vs Total Purchases\n",
    "axes[0, 1].scatter(df['age'], df['total_purchases'], alpha=0.6)\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].set_ylabel('Total Purchases')\n",
    "axes[0, 1].set_title('Age vs Total Purchases')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Years as Customer vs Satisfaction Score\n",
    "axes[1, 0].scatter(df['years_as_customer'], df['satisfaction_score'], alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Years as Customer')\n",
    "axes[1, 0].set_ylabel('Satisfaction Score')\n",
    "axes[1, 0].set_title('Years as Customer vs Satisfaction Score')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average Order Value vs Total Purchases\n",
    "axes[1, 1].scatter(df['avg_order_value'], df['total_purchases'], alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Average Order Value')\n",
    "axes[1, 1].set_ylabel('Total Purchases')\n",
    "axes[1, 1].set_title('Average Order Value vs Total Purchases')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Group Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spending patterns by gender\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "df.boxplot(column='total_spending', by='gender', ax=plt.gca())\n",
    "plt.title('Total Spending by Gender')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "df.boxplot(column='income', by='education', ax=plt.gca())\n",
    "plt.title('Income by Education Level')\n",
    "plt.suptitle('')  # Remove default title\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "df.boxplot(column='satisfaction_score', by='city_tier', ax=plt.gca())\n",
    "plt.title('Satisfaction Score by City Tier')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group statistics\n",
    "print(\"Average Total Spending by Gender:\")\n",
    "gender_spending = df.groupby('gender')['total_spending'].agg(['mean', 'median', 'std']).round(2)\n",
    "print(gender_spending)\n",
    "\n",
    "print(\"\\nAverage Income by Education Level:\")\n",
    "education_income = df.groupby('education')['income'].agg(['mean', 'median', 'std']).round(2)\n",
    "print(education_income)\n",
    "\n",
    "print(\"\\nAverage Satisfaction Score by City Tier:\")\n",
    "city_satisfaction = df.groupby('city_tier')['satisfaction_score'].agg(['mean', 'median', 'std']).round(2)\n",
    "print(city_satisfaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Advanced Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot for key numerical variables\n",
    "key_vars = ['age', 'income', 'total_spending', 'satisfaction_score']\n",
    "sns.pairplot(df[key_vars + ['gender']].dropna(), hue='gender', diag_kind='hist')\n",
    "plt.suptitle('Pair Plot of Key Variables by Gender', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create customer segments based on spending and purchases\n",
    "df['spending_category'] = pd.cut(df['total_spending'], \n",
    "                                bins=[0, df['total_spending'].quantile(0.33), \n",
    "                                     df['total_spending'].quantile(0.67), \n",
    "                                     df['total_spending'].max()],\n",
    "                                labels=['Low Spender', 'Medium Spender', 'High Spender'])\n",
    "\n",
    "# Visualize segments\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "segment_counts = df['spending_category'].value_counts()\n",
    "plt.pie(segment_counts.values, labels=segment_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Customer Segments by Spending')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "sns.boxplot(data=df, x='spending_category', y='satisfaction_score')\n",
    "plt.title('Satisfaction Score by Spending Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(data=df, x='spending_category', y='years_as_customer')\n",
    "plt.title('Customer Tenure by Spending Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Key Insights and Findings\n",
    "Let's summarize our key findings from the EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate key business metrics\n",
    "print(\"=== KEY BUSINESS INSIGHTS ===\")\n",
    "print(f\"\\n📊 Dataset Overview:\")\n",
    "print(f\"   • Total customers analyzed: {len(df):,}\")\n",
    "print(f\"   • Data quality: {((len(df) - df.isnull().sum().sum()) / (len(df) * len(df.columns)) * 100):.1f}% complete\")\n",
    "\n",
    "print(f\"\\n💰 Financial Metrics:\")\n",
    "print(f\"   • Average customer income: ${df['income'].mean():,.0f}\")\n",
    "print(f\"   • Average total spending: ${df['total_spending'].mean():,.0f}\")\n",
    "print(f\"   • Average order value: ${df['avg_order_value'].mean():.0f}\")\n",
    "print(f\"   • Total revenue: ${df['total_spending'].sum():,.0f}\")\n",
    "\n",
    "print(f\"\\n👥 Customer Demographics:\")\n",
    "print(f\"   • Average age: {df['age'].mean():.1f} years\")\n",
    "print(f\"   • Gender distribution: {dict(df['gender'].value_counts())}\")\n",
    "print(f\"   • Average customer tenure: {df['years_as_customer'].mean():.1f} years\")\n",
    "\n",
    "print(f\"\\n😊 Customer Satisfaction:\")\n",
    "print(f\"   • Average satisfaction score: {df['satisfaction_score'].mean():.1f}/10\")\n",
    "print(f\"   • Highly satisfied customers (>8): {len(df[df['satisfaction_score'] > 8])}/{len(df.dropna(subset=['satisfaction_score']))} ({len(df[df['satisfaction_score'] > 8])/len(df.dropna(subset=['satisfaction_score']))*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🎯 Customer Segments:\")\n",
    "segment_stats = df.groupby('spending_category').agg({\n",
    "    'total_spending': 'mean',\n",
    "    'satisfaction_score': 'mean',\n",
    "    'years_as_customer': 'mean'\n",
    "}).round(2)\n",
    "for segment in segment_stats.index:\n",
    "    count = len(df[df['spending_category'] == segment])\n",
    "    print(f\"   • {segment}: {count} customers ({count/len(df)*100:.1f}%)\")\n",
    "    print(f\"     - Avg spending: ${segment_stats.loc[segment, 'total_spending']:,.0f}\")\n",
    "    print(f\"     - Avg satisfaction: {segment_stats.loc[segment, 'satisfaction_score']:.1f}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Your Turn to Explore!\n",
    "Now it's your turn to practice EDA skills. Complete the following tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Create a new visualization\n",
    "Create a visualization that shows the relationship between education level and average order value. What insights can you draw?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 1\n",
    "# Hint: Try using a bar plot or box plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Identify the most valuable customer segment\n",
    "Based on the data, identify which combination of characteristics (gender, education, city_tier) represents the most valuable customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Challenge 2\n",
    "# Hint: Use groupby with multiple columns and calculate mean total_spending\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Data quality recommendations\n",
    "Based on your analysis, what recommendations would you make to improve data quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your recommendations here:**\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed a comprehensive EDA. Here's what you've learned:\n",
    "\n",
    "### ✅ Key Skills Mastered:\n",
    "1. **Data Loading and Inspection**: Using pandas to load and examine datasets\n",
    "2. **Data Quality Assessment**: Identifying missing values, duplicates, and outliers\n",
    "3. **Summary Statistics**: Calculating and interpreting descriptive statistics\n",
    "4. **Data Visualization**: Creating effective plots with matplotlib and seaborn\n",
    "5. **Pattern Recognition**: Identifying relationships and trends in data\n",
    "6. **Business Insights**: Translating data findings into actionable insights\n",
    "\n",
    "### 🔍 EDA Best Practices:\n",
    "- Always start with basic data inspection (`head()`, `info()`, `describe()`)\n",
    "- Check for data quality issues before analysis\n",
    "- Use appropriate visualizations for different data types\n",
    "- Look for patterns, outliers, and relationships\n",
    "- Document your findings and insights\n",
    "- Consider business context when interpreting results\n",
    "\n",
    "### 🚀 Next Steps:\n",
    "In the next lab, we'll learn how to clean and prepare this data for machine learning by:\n",
    "- Handling missing values\n",
    "- Encoding categorical variables\n",
    "- Scaling numerical features\n",
    "- Creating new features (feature engineering)\n",
    "\n",
    "### 📚 Additional Resources:\n",
    "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/)\n",
    "- [Seaborn Tutorial](https://seaborn.pydata.org/tutorial.html)\n",
    "- [EDA Best Practices](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

